{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_apriltags import Detector, Detection\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "from math import atan2, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory C:/Users/hbass/Desktop/SP2020 Research/Subject1/001/frames failed\n",
      "Extracted 1 frames from C:/Users/hbass/Desktop/SP2020 Research/Subject1/001/world.mp4.\n",
      "0\n",
      "Detected 0 tags in 0 frames.\n",
      "Found IDs of [].\n"
     ]
    }
   ],
   "source": [
    "#manual_detection.py\n",
    "#ignore for use on local machine, unfeasible to run without overclocking, even so, takes unreasonable amount of time\n",
    "\n",
    "\n",
    "def extract_frames(video_path: str, frames_path: str) -> None:\n",
    "    \"\"\"Convert a video (mp4 or similar) into a series of individual PNG frames.\n",
    "    Make sure to create a directory to store the frames before running this function.\n",
    "    Args:\n",
    "        video_path (str): filepath to the video being converted\n",
    "        frames_path (str): filepath to the target directory that will contain the extract frames\n",
    "    \"\"\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = 1\n",
    "\n",
    "    # Basically just using OpenCV's tools\n",
    "    while success:\n",
    "        success, frame = video.read()\n",
    "        cv2.imwrite(f'{frames_path}/frame{count}.png', frame)\n",
    "        count += 1\n",
    "\n",
    "    # Optional print statement\n",
    "    print(f'Extracted {count} frames from {video_path}.')\n",
    "\n",
    "\n",
    "def detect_tags(frames_path: str, aperture=11, visualize=False) -> Tuple[List[List[Dict[str, Any]]], Dict[int, int]]:\n",
    "    \"\"\"Detect all tags (Apriltags3) found in a folder of PNG files and return (1) a list of tag objects\n",
    "    for preprocessing and (2) a dictionary containing the frequency that each tag ID appeared\n",
    "    Args:n\n",
    "        frames_path (str): path to the directory containing PNG images\n",
    "        aperture (int):\n",
    "        visualize (bool):\n",
    "    Returns:\n",
    "        frames (List[Dict[str, Any]]): list of objects containing id (int), centroid (np.array[int]) and corners (np.array[int])\n",
    "        tag_ds (Dict[int, int]): dictionary mapping tag IDs to frequency of tag across all images\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    frames = []\n",
    "    tag_ids = defaultdict(int)\n",
    "    at_detector = Detector()\n",
    "\n",
    "    # Sort by index in.../frame<index>.png\n",
    "    all_images = sorted(glob(f'{frames_path}/*.png'), key=lambda f: int(os.path.basename(f)[5:-4]))\n",
    "\n",
    "    # Deleted last image after out of range error popped up\n",
    "    # TODO: but not analyzing last 2 frames?\n",
    "    # Feb 21: commented out deleting last frame, altho this was important when you have all frames in folder\n",
    "    print(len(all_images))\n",
    "    if len(all_images) > 1:\n",
    "        all_images = all_images[:-1]\n",
    "\n",
    "    num_images = len(all_images)\n",
    "    #print_progress_bar(0, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    # Iterate thru all PNG images in frames_path\n",
    "    for i, img_path in enumerate(all_images):\n",
    "        # Create a grayscale 2D NumPy array for Detector.detect()\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if type(img) == np.ndarray:\n",
    "            tags_in_framex = []\n",
    "            for tag in at_detector.detect(img):\n",
    "                # Increment frequency\n",
    "                tag_ids[tag.tag_id] += 1\n",
    "                # r = np.roll(np.float32(img), tag.homography + 1, axis=0)\n",
    "                # Add to frames in following format - feel free to adjust\n",
    "                tags_in_framex.append({\n",
    "                    'id': tag.tag_id,\n",
    "                    'id_confidence': tag.decision_margin,\n",
    "                    'soft_id': tag.tag_id,\n",
    "                    'perimeter': 100, #cv2.arcLength(img, closed=True),\n",
    "                    'centroid': tag.center,\n",
    "                    'verts': tag.corners,\n",
    "                    'frames_since_true_detection': 0\n",
    "                })\n",
    "\n",
    "                # {'id': msg, 'id_confidence': id_confidence, 'verts': r.tolist(), 'soft_id': soft_msg,\n",
    "                #  'perimeter': cv2.arcLength(r, closed=True), 'centroid': centroid.tolist(),\n",
    "                #  \"frames_since_true_detection\": 0}\n",
    "            frames.append(tags_in_framex)\n",
    "        time.sleep(0.01)\n",
    "        print_progress_bar(i + 1, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    return frames, dict(tag_ids)\n",
    "\n",
    "\n",
    "def print_progress_bar (iteration, total, prefix ='', suffix ='', decimals = 1, length = 100, fill ='â–ˆ', printEnd =\"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    sys.stdout.write('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix))\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define path folder\n",
    "    # test path 1 from 10/7 simulator recording - not all tags are id b/c lighting\n",
    "    # path = \"/home/whitney/Teresa/demos/surfaceTestUpdatedTags/001\"\n",
    "    # test path 2 from 10/7 simulator recording - not all tags are id b/c lighting\n",
    "    # path = \"/home/whitney/Teresa/demos/surfaceTestUpdatedTags/002\"\n",
    "    # test path 3 from prev recording at desk with reflection - all tags id success\n",
    "    # path = '/home/whitney/Teresa/demos/testingSurfaces_Teresa'\n",
    "    # test path to id tags\n",
    "    # path = '/home/whitney/Teresa/demos/idtags/013'\n",
    "    # test path 4 from 10/8 recording at lab with good lighting\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceTestUpdatedTagsLab/014'\n",
    "    # test path from 10/15 recording at lab with eye & good lighting\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceTestLabEye/000'\n",
    "    # test path from 10/17 recording at lab with calibration\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceANDcalibration_3screens'\n",
    "    # test path from 10/25 recording\n",
    "    #path = '/media/whitney/New Volume/Teresa/SD_grant_EM/Eye_Recordings/Subject1/001'\n",
    "    path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/001'\n",
    "    # test path from 11/4 recording\n",
    "    #path = '/home/whitney/Teresa/demos/2019_11_04/000'\n",
    "    # id tag\n",
    "    # path = '/home/whitney/recordings/2019_11_01/014'\n",
    "\n",
    "    # Create video path\n",
    "    video_path = path + \"/world.mp4\"\n",
    "    # Create frame path using OS package\n",
    "    # Define the name of the directory to be created\n",
    "    frames_path = path + \"/frames\"\n",
    "    try:\n",
    "        if not os.path.exists(frames_path):\n",
    "            os.mkdir(frames_path)\n",
    "        else:\n",
    "            print(\"Successfully created the directory %s \" % frames_path)\n",
    "    except OSError:\n",
    "        print(\"Creation of the directory %s failed\" % frames_path)\n",
    "    # Detect tags in frames\n",
    "    extract_frames(video_path, frames_path)\n",
    "    frames, tag_ids = detect_tags(frames_path)\n",
    "\n",
    "    # Descriptive print statements\n",
    "    tag_count = sum(count for count in tag_ids.values())\n",
    "    print(f'Detected {tag_count} tags in {len(frames)} frames.')\n",
    "    print(f'Found IDs of {list(tag_ids.keys())}.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tags_test(frames_path: str, aperture=11, visualize=False) -> Tuple[List[List[Dict[str, Any]]], Dict[int, int]]:\n",
    "    \"\"\"Detect all tags (Apriltags3) found in a folder of PNG files and return (1) a list of tag objects\n",
    "    for preprocessing and (2) a dictionary containing the frequency that each tag ID appeared\n",
    "    Args:n\n",
    "        frames_path (str): path to the directory containing PNG images\n",
    "        aperture (int):\n",
    "        visualize (bool):\n",
    "    Returns:\n",
    "        frames (List[Dict[str, Any]]): list of objects containing id (int), centroid (np.array[int]) and corners (np.array[int])\n",
    "        tag_ds (Dict[int, int]): dictionary mapping tag IDs to frequency of tag across all images\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    frames = []\n",
    "    tag_ids = defaultdict(int)\n",
    "    at_detector = Detector()\n",
    "\n",
    "    # Sort by index in.../frame<index>.png\n",
    "    all_images = sorted(glob(f'{frames_path}/*.png'), key=lambda f: int(os.path.basename(f)[5:-4]))\n",
    "\n",
    "    # Deleted last image after out of range error popped up\n",
    "    # TODO: but not analyzing last 2 frames?\n",
    "    # Feb 21: commented out deleting last frame, altho this was important when you have all frames in folder\n",
    "    print(len(all_images))\n",
    "    if len(all_images) > 1:\n",
    "        all_images = all_images[:-1]\n",
    "\n",
    "    num_images = len(all_images)\n",
    "    print_progress_bar(0, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    # Iterate thru all PNG images in frames_path\n",
    "    for i, img_path in enumerate(all_images):\n",
    "        # Create a grayscale 2D NumPy array for Detector.detect()\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if type(img) == np.ndarray:\n",
    "            tags_in_framex = []\n",
    "            for tag in at_detector.detect(img):\n",
    "                # Increment frequency\n",
    "                tag_ids[tag.tag_id] += 1\n",
    "                # r = np.roll(np.float32(img), tag.homography + 1, axis=0)\n",
    "                # Add to frames in following format - feel free to adjust\n",
    "                tags_in_framex.append({\n",
    "                    'id': tag.tag_id,\n",
    "                    'id_confidence': tag.decision_margin,\n",
    "                    'soft_id': tag.tag_id,\n",
    "                    'perimeter': 100, #cv2.arcLength(img, closed=True),\n",
    "                    'centroid': tag.center,\n",
    "                    'verts': tag.corners,\n",
    "                    'frames_since_true_detection': 0\n",
    "                })\n",
    "\n",
    "                # {'id': msg, 'id_confidence': id_confidence, 'verts': r.tolist(), 'soft_id': soft_msg,\n",
    "                #  'perimeter': cv2.arcLength(r, closed=True), 'centroid': centroid.tolist(),\n",
    "                #  \"frames_since_true_detection\": 0}\n",
    "            frames.append(tags_in_framex)\n",
    "        time.sleep(0.01)\n",
    "        print_progress_bar(i + 1, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    return frames, dict(tag_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning my experimentation\n",
    "## Comments on each block for what it is used for. Do not run cells with ignore tag\n",
    "## Will clean and scale better after initial image proves feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "[{'id': 0, 'id_confidence': 99.00302124023438, 'soft_id': 0, 'perimeter': 100, 'centroid': array([355.7077122 , 220.49514557]), 'verts': array([[342.56369019, 236.55683899],\n",
      "       [369.70407104, 234.2318573 ],\n",
      "       [369.14724731, 204.07234192],\n",
      "       [341.66320801, 206.71118164]]), 'frames_since_true_detection': 0}, {'id': 1, 'id_confidence': 96.2584457397461, 'soft_id': 1, 'perimeter': 100, 'centroid': array([362.15848359, 351.72380703]), 'verts': array([[349.66488647, 366.97045898],\n",
      "       [376.85266113, 366.11090088],\n",
      "       [374.76672363, 336.33724976],\n",
      "       [347.39718628, 337.27099609]]), 'frames_since_true_detection': 0}, {'id': 2, 'id_confidence': 85.86864471435547, 'soft_id': 2, 'perimeter': 100, 'centroid': array([374.80622407, 477.69332171]), 'verts': array([[363.44937134, 491.2350769 ],\n",
      "       [389.72055054, 491.75695801],\n",
      "       [386.41009521, 463.85702515],\n",
      "       [359.70401001, 463.45251465]]), 'frames_since_true_detection': 0}, {'id': 3, 'id_confidence': 99.62937927246094, 'soft_id': 3, 'perimeter': 100, 'centroid': array([514.78063242, 479.03755271]), 'verts': array([[500.83267212, 493.3616333 ],\n",
      "       [530.77661133, 493.43069458],\n",
      "       [528.99859619, 464.43618774],\n",
      "       [498.6008606 , 464.47903442]]), 'frames_since_true_detection': 0}, {'id': 5, 'id_confidence': 101.18315124511719, 'soft_id': 5, 'perimeter': 100, 'centroid': array([665.59619704, 477.22884689]), 'verts': array([[650.16571045, 492.15579224],\n",
      "       [681.10949707, 491.16586304],\n",
      "       [680.94244385, 462.38339233],\n",
      "       [649.87756348, 463.10736084]]), 'frames_since_true_detection': 0}, {'id': 6, 'id_confidence': 87.85365295410156, 'soft_id': 6, 'perimeter': 100, 'centroid': array([812.30464889, 472.72394708]), 'verts': array([[797.19696045, 487.66009521],\n",
      "       [825.66931152, 486.29040527],\n",
      "       [827.50085449, 457.70028687],\n",
      "       [798.7355957 , 458.95001221]]), 'frames_since_true_detection': 0}, {'id': 7, 'id_confidence': 104.79814910888672, 'soft_id': 7, 'perimeter': 100, 'centroid': array([817.15998013, 343.63203354]), 'verts': array([[802.15020752, 359.21655273],\n",
      "       [831.45959473, 358.93811035],\n",
      "       [832.23144531, 327.98345947],\n",
      "       [802.72357178, 328.17953491]]), 'frames_since_true_detection': 0}, {'id': 8, 'id_confidence': 94.23983764648438, 'soft_id': 8, 'perimeter': 100, 'centroid': array([816.99070099, 209.07865746]), 'verts': array([[802.28314209, 224.22209167],\n",
      "       [832.19329834, 225.3377533 ],\n",
      "       [831.69720459, 193.93630981],\n",
      "       [801.65490723, 192.67710876]]), 'frames_since_true_detection': 0}, {'id': 9, 'id_confidence': 109.72357177734375, 'soft_id': 9, 'perimeter': 100, 'centroid': array([662.2222816 , 206.60730277]), 'verts': array([[646.56604004, 222.3427124 ],\n",
      "       [678.55212402, 222.18634033],\n",
      "       [678.21734619, 190.53135681],\n",
      "       [645.35894775, 190.51930237]]), 'frames_since_true_detection': 0}, {'id': 11, 'id_confidence': 105.31678771972656, 'soft_id': 11, 'perimeter': 100, 'centroid': array([502.67909167, 210.40346754]), 'verts': array([[487.15484619, 226.86436462],\n",
      "       [518.83630371, 225.40910339],\n",
      "       [518.30151367, 193.83847046],\n",
      "       [486.4838562 , 195.36251831]]), 'frames_since_true_detection': 0}]\n"
     ]
    }
   ],
   "source": [
    "#returns the output of the detect tags on the specific image 4743\n",
    "frame, tag_ids = detect_tags_test('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected')\n",
    "print(frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets what attribute is of the dictionary generated by detect tags a user needs\n",
    "#for instance, I needed the centers of each tag and the corners to crop the image, so this function acquires them from the\n",
    "#return call of detect tags\n",
    "def attribute(frame, feature):\n",
    "    qr_codes = frame[0]\n",
    "    attributes = []\n",
    "    for i in range(len(qr_codes)):\n",
    "        qr_code = qr_codes[i]\n",
    "        print(feature + ' of tag id ' + str(i) + \":\", qr_code[feature])\n",
    "        attributes.append(qr_code[feature])\n",
    "    return attributes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroid of tag id 0: [355.7077122  220.49514557]\n",
      "centroid of tag id 1: [362.15848359 351.72380703]\n",
      "centroid of tag id 2: [374.80622407 477.69332171]\n",
      "centroid of tag id 3: [514.78063242 479.03755271]\n",
      "centroid of tag id 4: [665.59619704 477.22884689]\n",
      "centroid of tag id 5: [812.30464889 472.72394708]\n",
      "centroid of tag id 6: [817.15998013 343.63203354]\n",
      "centroid of tag id 7: [816.99070099 209.07865746]\n",
      "centroid of tag id 8: [662.2222816  206.60730277]\n",
      "centroid of tag id 9: [502.67909167 210.40346754]\n"
     ]
    }
   ],
   "source": [
    "#attains the centers of each QR code of the frame; not sorted in any order\n",
    "centers = attribute(frame, 'centroid')\n",
    "#comment for myself: from corners, iterate through and find the corners of highest top left, highest top right, lowest bottom left, lowest bottom right, and then draw a box around the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verts of tag id 0: [[342.56369019 236.55683899]\n",
      " [369.70407104 234.2318573 ]\n",
      " [369.14724731 204.07234192]\n",
      " [341.66320801 206.71118164]]\n",
      "verts of tag id 1: [[349.66488647 366.97045898]\n",
      " [376.85266113 366.11090088]\n",
      " [374.76672363 336.33724976]\n",
      " [347.39718628 337.27099609]]\n",
      "verts of tag id 2: [[363.44937134 491.2350769 ]\n",
      " [389.72055054 491.75695801]\n",
      " [386.41009521 463.85702515]\n",
      " [359.70401001 463.45251465]]\n",
      "verts of tag id 3: [[500.83267212 493.3616333 ]\n",
      " [530.77661133 493.43069458]\n",
      " [528.99859619 464.43618774]\n",
      " [498.6008606  464.47903442]]\n",
      "verts of tag id 4: [[650.16571045 492.15579224]\n",
      " [681.10949707 491.16586304]\n",
      " [680.94244385 462.38339233]\n",
      " [649.87756348 463.10736084]]\n",
      "verts of tag id 5: [[797.19696045 487.66009521]\n",
      " [825.66931152 486.29040527]\n",
      " [827.50085449 457.70028687]\n",
      " [798.7355957  458.95001221]]\n",
      "verts of tag id 6: [[802.15020752 359.21655273]\n",
      " [831.45959473 358.93811035]\n",
      " [832.23144531 327.98345947]\n",
      " [802.72357178 328.17953491]]\n",
      "verts of tag id 7: [[802.28314209 224.22209167]\n",
      " [832.19329834 225.3377533 ]\n",
      " [831.69720459 193.93630981]\n",
      " [801.65490723 192.67710876]]\n",
      "verts of tag id 8: [[646.56604004 222.3427124 ]\n",
      " [678.55212402 222.18634033]\n",
      " [678.21734619 190.53135681]\n",
      " [645.35894775 190.51930237]]\n",
      "verts of tag id 9: [[487.15484619 226.86436462]\n",
      " [518.83630371 225.40910339]\n",
      " [518.30151367 193.83847046]\n",
      " [486.4838562  195.36251831]]\n"
     ]
    }
   ],
   "source": [
    "#attains the corners for each of the QR codes in the frame; not sorted in any order\n",
    "corners = attribute(frame, 'verts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed anymore since we use a bitwise masking procedure to crop the image. This was done using the\n",
    "#cv2 rectangle function which requires an upper left and bottom right corner. Keeping for future reference on how to\n",
    "#use the cv2 rectangle function.\n",
    "\n",
    "#TESTING FOR OPTIMAL POINT: drawing of the rectangle using cv2, need an upper left and bottom right corner\n",
    "for i in range(len(corners)):\n",
    "    sq = corners[i]\n",
    "    for j in range(len(sq)):\n",
    "        coord = sq[j]\n",
    "        path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "        image = cv2.imread(path)\n",
    "        window_name = 'Box'\n",
    "        start_point= (341,206)\n",
    "        end_point = (int(coord[0]), int(coord[1]))\n",
    "        print(end_point)\n",
    "        color = (255,0,0)\n",
    "        thickness = 2\n",
    "        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        cv2.imshow(window_name, image);\n",
    "        cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed given bitwise mask below but good reference.\n",
    "#The drawing of the image generated from the best coordinates from the cell above\n",
    "#DRAWING HERE\n",
    "path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "image = cv2.imread(path)\n",
    "window_name = 'Box'\n",
    "start_point= (341,206)\n",
    "end_point = (825, 486)\n",
    "color = (255,0,0)\n",
    "thickness = 2\n",
    "image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "cv2.imshow(window_name, image);\n",
    "cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through all centroids of the QR codes and generates the lines create the XY grid through the center of the screen.\n",
    "#When running, hit a button on the keyboard to filter through all combinations until an appropriate one is found.\n",
    "#Prints out the coordinates that were used.\n",
    "\n",
    "#Testing lines for appropriate XY grid \n",
    "#centroid for xy grid\n",
    "for centroid1 in centers:\n",
    "    for centroid2 in centers:\n",
    "        path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "        image = cv2.imread(path)\n",
    "        window_name = 'Box'\n",
    "        start_point= (int(centroid1[0]),int(centroid1[1]))\n",
    "        end_point = (int(centroid2[0]),int(centroid2[1]))\n",
    "        print(start_point, end_point)\n",
    "        color = (255,0,0)\n",
    "        thickness = 2\n",
    "        image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "        cv2.imshow(window_name, image);\n",
    "        cv2.waitKey(0);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bitwise masking that crops the image based on just what we want to see on the screen. Utilzes the corner\n",
    "#coordiates from previous cell computations, and then filters through to see which is the best mask.\n",
    "#Hit a keyboard button to filter through images.\n",
    "\n",
    "#Testing for the cropping of the mask image \n",
    "for i in range(len(corners)):\n",
    "    sq = corners[i]\n",
    "    for j in range(len(sq)):\n",
    "        coord = sq[j]\n",
    "        x = int(coord[0])\n",
    "        y = int(coord[1])\n",
    "        print(x,y)\n",
    "        im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "        mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "        roi_corners = np.array([[(341, 206), (x,y), (831, 193), (825,486)]], dtype=np.int32)\n",
    "        channel_count = im.shape[2]\n",
    "        ignore_mask_color = (255,)*channel_count\n",
    "        cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "        masked_image = cv2.bitwise_and(im, mask)\n",
    "        cv2.imshow('masked', masked_image);\n",
    "        cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the ROI_CORNERS parameter of the masking requires a specific ordering to crop the image, we need to find\n",
    "#the best permutation of the coordinates calculated from before to generate the image we need\n",
    "#Hit a button on the keyboard to filter through the images.\n",
    "#Prints out coordinate.\n",
    "\n",
    "#finding the appropriate permutation of the pixels that crop the screen\n",
    "points = [(341, 206), (363, 491), (831, 193), (825,486)]\n",
    "for i in range(len(points)):\n",
    "    for j in range(len(points)):\n",
    "        for k in range(len(points)):\n",
    "            for n in range(len(points)):\n",
    "                im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "                mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "                print(points[i], points[j], points[k], points[n])\n",
    "                roi_corners = np.array([[points[i], points[j], points[k], points[n]]], dtype=np.int32)\n",
    "                channel_count = im.shape[2]\n",
    "                ignore_mask_color = (255,)*channel_count\n",
    "                cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "                masked_image = cv2.bitwise_and(im, mask)\n",
    "                cv2.imshow('masked', masked_image);\n",
    "                cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the midpoint between two pixel points, and rounds them to nearest integer for use in drawing programs cv2line/circle\n",
    "def midpoint(point1, point2):\n",
    "    return (int((point1[0] + point2[0]) / 2) , int((point1[1] + point2[1]) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to generate the XY grid on the specific image \n",
    "#optimal points found/masking the image to crop the screen\n",
    "im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "roi_corners = np.array([[(341,206), (831,193), (825,486), (363,491)]], dtype=np.int32)\n",
    "channel_count = im.shape[2]\n",
    "ignore_mask_color = (255,)*channel_count\n",
    "cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "masked_image = cv2.bitwise_and(im, mask)\n",
    "\n",
    "#corners of the rectangle around the blob\n",
    "upper_left = (600,375)\n",
    "upper_right = (672, 375)\n",
    "bottom_right = (672, 439)\n",
    "bottom_left = (600, 439)\n",
    "\n",
    "#drawing code to draw on the cropped image\n",
    "start_point = (362 ,352)\n",
    "end_point = (817, 343)\n",
    "colour = (0,255,0)\n",
    "thickness = 2\n",
    "#X-AXIS\n",
    "masked_image = cv2.line(masked_image, start_point, end_point, colour, thickness)\n",
    "#Y-AXIS \n",
    "top_vert_line = midpoint((666,477), (515, 479))\n",
    "bot_vert_line = midpoint((662,207), (503,210))\n",
    "cv2.line(masked_image, top_vert_line, bot_vert_line , colour, thickness)\n",
    "#ORIGIN\n",
    "cv2.circle(masked_image, (586, 348), 2, (0,0,255), thickness)\n",
    "#Testing to box around the blob\n",
    "cv2.rectangle(masked_image, (600, 375), (672, 439), (255,0,0), thickness)\n",
    "\n",
    "#center of the blob\n",
    "cv2.line(masked_image, midpoint(upper_left, bottom_left), midpoint(upper_right, bottom_right), (0,0,255), thickness)\n",
    "cv2.line(masked_image, midpoint(upper_left, upper_right), midpoint(bottom_left, bottom_right), (0,0,255), thickness)\n",
    "cv2.circle(masked_image, (636,407), 2, colour, thickness)\n",
    "cv2.imshow('XY_PLANE', masked_image);\n",
    "cv2.waitKey(1000);\n",
    "#cv2.imshow('mask',masked_image);\n",
    "#cv2.imsave('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/masked_image.jpg', masked_image)\n",
    "#cv2.waitKey(0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing cell to find best fit origin\n",
    "start_point = (362 ,352)\n",
    "end_point = (817, 343)\n",
    "top_vert_line = midpoint((666,477), (515, 479))\n",
    "bot_vert_line = midpoint((662,207), (503,210))\n",
    "origin_y = midpoint(top_vert_line, bot_vert_line)\n",
    "origin_x = midpoint(start_point, end_point)\n",
    "#found by tuning the values computed above\n",
    "origin = (586,348)\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pixel convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the stimulus is 72 pixels and 0.8756694059682902 visual degrees\n"
     ]
    }
   ],
   "source": [
    "#pixel to degree\n",
    "#TODO: figure out what size_in_px represents\n",
    "h = 19.65\n",
    "d = 57.3\n",
    "r = 1600\n",
    "size_in_px = 72\n",
    "deg_per_px = degrees(atan2(.5*h, d)) / (.5*r)\n",
    "size_in_deg = size_in_px * deg_per_px\n",
    "print('The size of the stimulus is %s pixels and %s visual degrees' \\\n",
    "    % (size_in_px, size_in_deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the stimulus is 328.89124370119777 pixels and 4 visual degrees\n"
     ]
    }
   ],
   "source": [
    "h = 19.65\n",
    "d = 57.3\n",
    "r = 1600\n",
    "#TODO: figure out what size_in_deg represents for experiment\n",
    "size_in_deg = \n",
    "deg_per_px = degrees(atan2(.5*h, d)) / (.5*r)\n",
    "size_in_px = size_in_deg / deg_per_px\n",
    "print ('The size of the stimulus is %s pixels and %s visual degrees' \\\n",
    "    % (size_in_px, size_in_deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
