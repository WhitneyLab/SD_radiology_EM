{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_apriltags import Detector, Detection\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual_detection.py\n",
    "#ignore for use on local machine, unfeasible to run without overclocking, even so, takes unreasonable amount of time\n",
    "\n",
    "\n",
    "def extract_frames(video_path: str, frames_path: str) -> None:\n",
    "    \"\"\"Convert a video (mp4 or similar) into a series of individual PNG frames.\n",
    "    Make sure to create a directory to store the frames before running this function.\n",
    "    Args:\n",
    "        video_path (str): filepath to the video being converted\n",
    "        frames_path (str): filepath to the target directory that will contain the extract frames\n",
    "    \"\"\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = 1\n",
    "\n",
    "    # Basically just using OpenCV's tools\n",
    "    while success:\n",
    "        success, frame = video.read()\n",
    "        cv2.imwrite(f'{frames_path}/frame{count}.png', frame)\n",
    "        count += 1\n",
    "\n",
    "    # Optional print statement\n",
    "    print(f'Extracted {count} frames from {video_path}.')\n",
    "\n",
    "\n",
    "def detect_tags(frames_path: str, aperture=11, visualize=False) -> Tuple[List[List[Dict[str, Any]]], Dict[int, int]]:\n",
    "    \"\"\"Detect all tags (Apriltags3) found in a folder of PNG files and return (1) a list of tag objects\n",
    "    for preprocessing and (2) a dictionary containing the frequency that each tag ID appeared\n",
    "    Args:n\n",
    "        frames_path (str): path to the directory containing PNG images\n",
    "        aperture (int):\n",
    "        visualize (bool):\n",
    "    Returns:\n",
    "        frames (List[Dict[str, Any]]): list of objects containing id (int), centroid (np.array[int]) and corners (np.array[int])\n",
    "        tag_ds (Dict[int, int]): dictionary mapping tag IDs to frequency of tag across all images\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    frames = []\n",
    "    tag_ids = defaultdict(int)\n",
    "    at_detector = Detector()\n",
    "\n",
    "    # Sort by index in.../frame<index>.png\n",
    "    all_images = sorted(glob(f'{frames_path}/*.png'), key=lambda f: int(os.path.basename(f)[5:-4]))\n",
    "\n",
    "    # Deleted last image after out of range error popped up\n",
    "    # TODO: but not analyzing last 2 frames?\n",
    "    # Feb 21: commented out deleting last frame, altho this was important when you have all frames in folder\n",
    "    print(len(all_images))\n",
    "    if len(all_images) > 1:\n",
    "        all_images = all_images[:-1]\n",
    "\n",
    "    num_images = len(all_images)\n",
    "    #print_progress_bar(0, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    # Iterate thru all PNG images in frames_path\n",
    "    for i, img_path in enumerate(all_images):\n",
    "        # Create a grayscale 2D NumPy array for Detector.detect()\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if type(img) == np.ndarray:\n",
    "            tags_in_framex = []\n",
    "            for tag in at_detector.detect(img):\n",
    "                # Increment frequency\n",
    "                tag_ids[tag.tag_id] += 1\n",
    "                # r = np.roll(np.float32(img), tag.homography + 1, axis=0)\n",
    "                # Add to frames in following format - feel free to adjust\n",
    "                tags_in_framex.append({\n",
    "                    'id': tag.tag_id,\n",
    "                    'id_confidence': tag.decision_margin,\n",
    "                    'soft_id': tag.tag_id,\n",
    "                    'perimeter': 100, #cv2.arcLength(img, closed=True),\n",
    "                    'centroid': tag.center,\n",
    "                    'verts': tag.corners,\n",
    "                    'frames_since_true_detection': 0\n",
    "                })\n",
    "\n",
    "                # {'id': msg, 'id_confidence': id_confidence, 'verts': r.tolist(), 'soft_id': soft_msg,\n",
    "                #  'perimeter': cv2.arcLength(r, closed=True), 'centroid': centroid.tolist(),\n",
    "                #  \"frames_since_true_detection\": 0}\n",
    "            frames.append(tags_in_framex)\n",
    "        time.sleep(0.01)\n",
    "        print_progress_bar(i + 1, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    return frames, dict(tag_ids)\n",
    "\n",
    "\n",
    "def print_progress_bar (iteration, total, prefix ='', suffix ='', decimals = 1, length = 100, fill ='â–ˆ', printEnd =\"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    sys.stdout.write('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix))\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define path folder\n",
    "    # test path 1 from 10/7 simulator recording - not all tags are id b/c lighting\n",
    "    # path = \"/home/whitney/Teresa/demos/surfaceTestUpdatedTags/001\"\n",
    "    # test path 2 from 10/7 simulator recording - not all tags are id b/c lighting\n",
    "    # path = \"/home/whitney/Teresa/demos/surfaceTestUpdatedTags/002\"\n",
    "    # test path 3 from prev recording at desk with reflection - all tags id success\n",
    "    # path = '/home/whitney/Teresa/demos/testingSurfaces_Teresa'\n",
    "    # test path to id tags\n",
    "    # path = '/home/whitney/Teresa/demos/idtags/013'\n",
    "    # test path 4 from 10/8 recording at lab with good lighting\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceTestUpdatedTagsLab/014'\n",
    "    # test path from 10/15 recording at lab with eye & good lighting\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceTestLabEye/000'\n",
    "    # test path from 10/17 recording at lab with calibration\n",
    "    # path = '/home/whitney/Teresa/demos/surfaceANDcalibration_3screens'\n",
    "    # test path from 10/25 recording\n",
    "    #path = '/media/whitney/New Volume/Teresa/SD_grant_EM/Eye_Recordings/Subject1/001'\n",
    "    path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/001'\n",
    "    # test path from 11/4 recording\n",
    "    #path = '/home/whitney/Teresa/demos/2019_11_04/000'\n",
    "    # id tag\n",
    "    # path = '/home/whitney/recordings/2019_11_01/014'\n",
    "\n",
    "    # Create video path\n",
    "    video_path = path + \"/world.mp4\"\n",
    "    # Create frame path using OS package\n",
    "    # Define the name of the directory to be created\n",
    "    frames_path = path + \"/frames\"\n",
    "    try:\n",
    "        if not os.path.exists(frames_path):\n",
    "            os.mkdir(frames_path)\n",
    "        else:\n",
    "            print(\"Successfully created the directory %s \" % frames_path)\n",
    "    except OSError:\n",
    "        print(\"Creation of the directory %s failed\" % frames_path)\n",
    "    # Detect tags in frames\n",
    "    extract_frames(video_path, frames_path)\n",
    "    frames, tag_ids = detect_tags(frames_path)\n",
    "\n",
    "    # Descriptive print statements\n",
    "    tag_count = sum(count for count in tag_ids.values())\n",
    "    print(f'Detected {tag_count} tags in {len(frames)} frames.')\n",
    "    print(f'Found IDs of {list(tag_ids.keys())}.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tags_test(frames_path: str, aperture=11, visualize=False) -> Tuple[List[List[Dict[str, Any]]], Dict[int, int]]:\n",
    "    \"\"\"Detect all tags (Apriltags3) found in a folder of PNG files and return (1) a list of tag objects\n",
    "    for preprocessing and (2) a dictionary containing the frequency that each tag ID appeared\n",
    "    Args:n\n",
    "        frames_path (str): path to the directory containing PNG images\n",
    "        aperture (int):\n",
    "        visualize (bool):\n",
    "    Returns:\n",
    "        frames (List[Dict[str, Any]]): list of objects containing id (int), centroid (np.array[int]) and corners (np.array[int])\n",
    "        tag_ds (Dict[int, int]): dictionary mapping tag IDs to frequency of tag across all images\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    frames = []\n",
    "    tag_ids = defaultdict(int)\n",
    "    at_detector = Detector()\n",
    "\n",
    "    # Sort by index in.../frame<index>.png\n",
    "    all_images = sorted(glob(f'{frames_path}/*.png'), key=lambda f: int(os.path.basename(f)[5:-4]))\n",
    "\n",
    "    # Deleted last image after out of range error popped up\n",
    "    # TODO: but not analyzing last 2 frames?\n",
    "    # Feb 21: commented out deleting last frame, altho this was important when you have all frames in folder\n",
    "    print(len(all_images))\n",
    "    if len(all_images) > 1:\n",
    "        all_images = all_images[:-1]\n",
    "\n",
    "    num_images = len(all_images)\n",
    "    print_progress_bar(0, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    # Iterate thru all PNG images in frames_path\n",
    "    for i, img_path in enumerate(all_images):\n",
    "        # Create a grayscale 2D NumPy array for Detector.detect()\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if type(img) == np.ndarray:\n",
    "            tags_in_framex = []\n",
    "            for tag in at_detector.detect(img):\n",
    "                # Increment frequency\n",
    "                tag_ids[tag.tag_id] += 1\n",
    "                # r = np.roll(np.float32(img), tag.homography + 1, axis=0)\n",
    "                # Add to frames in following format - feel free to adjust\n",
    "                tags_in_framex.append({\n",
    "                    'id': tag.tag_id,\n",
    "                    'id_confidence': tag.decision_margin,\n",
    "                    'soft_id': tag.tag_id,\n",
    "                    'perimeter': 100, #cv2.arcLength(img, closed=True),\n",
    "                    'centroid': tag.center,\n",
    "                    'verts': tag.corners,\n",
    "                    'frames_since_true_detection': 0\n",
    "                })\n",
    "\n",
    "                # {'id': msg, 'id_confidence': id_confidence, 'verts': r.tolist(), 'soft_id': soft_msg,\n",
    "                #  'perimeter': cv2.arcLength(r, closed=True), 'centroid': centroid.tolist(),\n",
    "                #  \"frames_since_true_detection\": 0}\n",
    "            frames.append(tags_in_framex)\n",
    "        time.sleep(0.01)\n",
    "        print_progress_bar(i + 1, num_images, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "    return frames, dict(tag_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning my experimentation\n",
    "## Comments on each block for what it is used for. Do not run cells with ignore tag\n",
    "## Will clean and scale better after initial image proves feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the output of the detect tags on the specific image 4743\n",
    "frame, tag_ids = detect_tags_test('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected')\n",
    "print(frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets what attribute is of the dictionary generated by detect tags a user needs\n",
    "#for instance, I needed the centers of each tag and the corners to crop the image, so this function acquires tgem from the\n",
    "#return call of detect tags\n",
    "def attribute(frame, feature):\n",
    "    qr_codes = frame[0]\n",
    "    attributes = []\n",
    "    for i in range(len(qr_codes)):\n",
    "        qr_code = qr_codes[i]\n",
    "        print(feature + ' of tag id ' + str(i) + \":\", qr_code[feature])\n",
    "        attributes.append(qr_code[feature])\n",
    "    return attributes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attains the centers of each QR code of the frame; not sorted in any order\n",
    "centers = attribute(frame, 'centroid')\n",
    "#comment for myself: from corners, iterate through and find the corners of highest top left, highest top right, lowest bottom left, lowest bottom right, and then draw a box around the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attains the corners for each of the QR codes in the frame; not sorted in any order\n",
    "corners = attribute(frame, 'verts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE\n",
    "#TESTING FOR OPTIMAL POINT: drawing of the rectangle using cv2, need an upper left and bottom right corner\n",
    "for i in range(len(corners)):\n",
    "    sq = corners[i]\n",
    "    for j in range(len(sq)):\n",
    "        coord = sq[j]\n",
    "        path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "        image = cv2.imread(path)\n",
    "        window_name = 'Box'\n",
    "        start_point= (341,206)\n",
    "        end_point = (int(coord[0]), int(coord[1]))\n",
    "        print(end_point)\n",
    "        color = (255,0,0)\n",
    "        thickness = 2\n",
    "        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        cv2.imshow(window_name, image);\n",
    "        cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE\n",
    "#DRAWING HERE\n",
    "path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "image = cv2.imread(path)\n",
    "window_name = 'Box'\n",
    "start_point= (341,206)\n",
    "end_point = (825, 486)\n",
    "color = (255,0,0)\n",
    "thickness = 2\n",
    "image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "cv2.imshow(window_name, image);\n",
    "cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE\n",
    "#Testing lines for appropriate XY grid \n",
    "#centroid for xy grid\n",
    "for centroid1 in centers:\n",
    "    for centroid2 in centers:\n",
    "        path = 'C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png'\n",
    "        image = cv2.imread(path)\n",
    "        window_name = 'Box'\n",
    "        start_point= (int(centroid1[0]),int(centroid1[1]))\n",
    "        end_point = (int(centroid2[0]),int(centroid2[1]))\n",
    "        print(start_point, end_point)\n",
    "        color = (255,0,0)\n",
    "        thickness = 2\n",
    "        image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "        cv2.imshow(window_name, image);\n",
    "        cv2.waitKey(0);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE\n",
    "#Testing for the cropping of the mask image \n",
    "for i in range(len(corners)):\n",
    "    sq = corners[i]\n",
    "    for j in range(len(sq)):\n",
    "        coord = sq[j]\n",
    "        x = int(coord[0])\n",
    "        y = int(coord[1])\n",
    "        print(x,y)\n",
    "        im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "        mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "        roi_corners = np.array([[(341, 206), (x,y), (831, 193), (825,486)]], dtype=np.int32)\n",
    "        channel_count = im.shape[2]\n",
    "        ignore_mask_color = (255,)*channel_count\n",
    "        cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "        masked_image = cv2.bitwise_and(im, mask)\n",
    "        cv2.imshow('masked', masked_image);\n",
    "        cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE\n",
    "#finding the appropriate permutation of the pixels that crop the screen\n",
    "points = [(341, 206), (363, 491), (831, 193), (825,486)]\n",
    "for i in range(len(points)):\n",
    "    for j in range(len(points)):\n",
    "        for k in range(len(points)):\n",
    "            for n in range(len(points)):\n",
    "                im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "                mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "                print(points[i], points[j], points[k], points[n])\n",
    "                roi_corners = np.array([[points[i], points[j], points[k], points[n]]], dtype=np.int32)\n",
    "                channel_count = im.shape[2]\n",
    "                ignore_mask_color = (255,)*channel_count\n",
    "                cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "                masked_image = cv2.bitwise_and(im, mask)\n",
    "                cv2.imshow('masked', masked_image);\n",
    "                cv2.waitKey(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the midpoint between two pixel points, and rounds them to nearest integer for use in drawing programs cv2line/circle\n",
    "def midpoint(point1, point2):\n",
    "    return (int((point1[0] + point2[0]) / 2) , int((point1[1] + point2[1]) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to generate the XY grid on the specific image \n",
    "#optimal points found/masking the image to crop the screen\n",
    "im = cv2.imread('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/frame7473.png')\n",
    "mask = np.zeros(im.shape, dtype=np.uint8)\n",
    "roi_corners = np.array([[(341,206), (831,193), (825,486), (363,491)]], dtype=np.int32)\n",
    "channel_count = im.shape[2]\n",
    "ignore_mask_color = (255,)*channel_count\n",
    "cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "masked_image = cv2.bitwise_and(im, mask)\n",
    "\n",
    "#drawing code to draw on the cropped image\n",
    "start_point = (362 ,352)\n",
    "end_point = (817, 343)\n",
    "colour = (0,255,0)\n",
    "thickness = 2\n",
    "#X-AXIS\n",
    "masked_image = cv2.line(masked_image, start_point, end_point, colour, thickness)\n",
    "#Y-AXIS \n",
    "top_vert_line = midpoint((666,477), (515, 479))\n",
    "bot_vert_line = midpoint((662,207), (503,210))\n",
    "cv2.line(masked_image, top_vert_line, bot_vert_line , colour, thickness)\n",
    "#ORIGIN\n",
    "cv2.circle(masked_image, (586, 348), 5, colour, thickness)\n",
    "#Testing to box around the blob\n",
    "cv2.circle(masked_image, (643, 415), 20, (255,0,0), thickness)\n",
    "\n",
    "cv2.imshow('XY_PLANE', masked_image);\n",
    "cv2.waitKey(0);\n",
    "#cv2.imshow('mask',masked_image);\n",
    "#cv2.imsave('C:/Users/hbass/Desktop/SP2020 Research/Subject1/Subject1/001/frames_selected/masked_image.jpg', masked_image)\n",
    "#cv2.waitKey(0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "source": [
    "def test_git():\n",
    "    print('lol')\n",
    "test_git()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing cell to find best fit origin\n",
    "start_point = (362 ,352)\n",
    "end_point = (817, 343)\n",
    "top_vert_line = midpoint((666,477), (515, 479))\n",
    "bot_vert_line = midpoint((662,207), (503,210))\n",
    "origin_y = midpoint(top_vert_line, bot_vert_line)\n",
    "origin_x = midpoint(start_point, end_point)\n",
    "#found by tuning the values computed above\n",
    "origin = (586,348)\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
